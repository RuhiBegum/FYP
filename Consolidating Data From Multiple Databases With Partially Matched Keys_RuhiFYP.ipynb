{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the relevant packages I will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pyodbc as p\n",
    "from nltk.stem import *\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to a specific Database. We will pass the serverName and database as parameters so users can define their own servers and databases. Makes it more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectToDB(serverName, database):\n",
    "    \n",
    "    sql_conn = p.connect('DRIVER={ODBC Driver 17 for SQL Server}; SERVER=' + serverName + ';DATABASE=' + database + ';Trusted_Connection=yes;')       \n",
    "    return sql_conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is a function that connects to the specified database (of users' choice) and then a retrieval query is executed and stored as a DataFrame. This is then written back to SSMS and the DataFrame is returned as an output. \n",
    "\n",
    "This code is only ever needed to when wanting to take a subset of a larger table and write to another place in the Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def writeDB(database):\n",
    "#    conn = connectToDB('DESKTOP-7CLEOQN\\SQLEXPRESS', database)\n",
    "#    query =  \"SELECT [CustomerID],[CustomerName],[CustomerReferenceNo],[AddressLine4] FROM dbo.Mock3Customer\"\n",
    "#    df = pd.read_sql(query, conn)\n",
    "    \n",
    "#    cursor = conn.cursor()\n",
    "    \n",
    "#    for index, row in df.iterrows():\n",
    "#        cursor.execute(\"INSERT INTO [Mock3].[dbo].[test3]([CustomerID],[CustomerName],[CustomerReferenceNo],[PostCode]) values (?,?,?,?)\",row['CustomerID'],row['CustomerName'],row['CustomerReferenceNo'],row['AddressLine4']) \n",
    "#        conn.commit()\n",
    "#    cursor.close()\n",
    "#    conn.close()\n",
    "    \n",
    "#    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function reads the desired query the user wishes to execute. For the purpose of my project, I will retrieve everything I wrote to the new test tables in the previous command for each of the databases and store them into a dataframe of its own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDB(database):\n",
    "    conn = connectToDB('DESKTOP-7CLEOQN\\SQLEXPRESS', database)\n",
    "    query =  \"SELECT * FROM dbo.test1\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "db1 = readDB('Mock')\n",
    "print(db1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the following code to simply make sure we are retrieving everyting we expect. We can manually cross-validate by opening SSMS and seeing the same data points. Since this is a self-created table, we are only working with upto 15 data points at most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDB(database):\n",
    "    conn = connectToDB('DESKTOP-7CLEOQN\\SQLEXPRESS', database)\n",
    "    query =  \"SELECT * FROM dbo.test2\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = readDB('Mock2')\n",
    "print(db2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDB(database):\n",
    "    conn = connectToDB('DESKTOP-7CLEOQN\\SQLEXPRESS', database)\n",
    "    query =  \"SELECT * FROM dbo.test3\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db3 = readDB('Mock3')\n",
    "print(db3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column by merging two columns. In this case we are merging the CustomerName column with the PostCode since this will form our new key and aggregating them initially makes the tokenisation easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1['name_postcode'] = db1[['CustomerName', 'AddressLine4']].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2['name_postcode'] = db2[['CustomerName', 'AddressLine4']].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code does a Left join on the two DataFrames (db1 and db2). We see that as it stands, without employing our tool capabilities; there is only one match - the database identifies only one entity to exist the same across both tables. \n",
    "100% identical match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inner_join = pd.merge(db1, db2, on='CustomerName', how='left')\n",
    "\n",
    "inner_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function performs normalisation which will be used on our tokens. It takes any string and removes any punctuation within the string. It then turns every character left into lowercase so there is no variation between 'and' and 'And'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_punctuation(text):\n",
    "    delim = dict.fromkeys(':,.?\\n-&\"?!')\n",
    "    string = ' '\n",
    "    sentence = [] \n",
    "  \n",
    "    for word in text: \n",
    "        if word not in delim: \n",
    "            sentence.append(word) \n",
    "        else:\n",
    "            sentence.append(' ')\n",
    "            \n",
    "    for i in sentence:\n",
    "        string += i\n",
    "\n",
    "    lower = string.lower() #make it all lowercase so we can avoid thinking about capitals (and/And)\n",
    "    return lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform tokenization (using NLTK library), remove stopwords etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_pc = db1['name_postcode']\n",
    "tokens = []\n",
    "\n",
    "for i in name_pc:\n",
    "    no_punc = strip_punctuation(i)\n",
    "    tokenize = word_tokenize(no_punc) #tokenization from the NLTK library\n",
    "    \n",
    "    #LIST of all company/customer name stopwords \n",
    "    stopwords = ['agency', 'gmbh', 'pa', 'assn', 'group', 'and', 'pc', 'hotel', 'pharmacy', 'assoc', 'hotels', 'plc', 'associates', 'inc', 'pllc', 'association', 'incorporated', 'restaurant', 'bank', 'international', 'sa', 'bv', 'intl', 'sales', 'co', 'limited', 'service', 'comp', 'llc', 'services', 'company', 'llp', 'store', 'corp', 'lp', 'svcs', 'corporation', 'ltd', 'travel', 'dmd', 'manufacturing', 'unlimited', 'enterprises', 'mfg']\n",
    "    \n",
    "    filtered_sentence = [w for w in tokenize if not w in stopwords] \n",
    "    tokens.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [''.join(i) for i in tokens]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1['newKeys'] = keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(db1) #This line of code is simply to show what we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can join the tables to see the effect of creating new keys, we will manually do the same for the entries in our second database table in this case db2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_pc2 = db2['name_postcode']\n",
    "tokens = []\n",
    "\n",
    "for i in name_pc2:\n",
    "    no_punc = strip_punctuation(i)\n",
    "    tokenize = word_tokenize(no_punc) #tokenization from the NLTK library\n",
    "    \n",
    "    #LIST of all company/customer name stopwords \n",
    "    stopwords = ['agency', 'ltd', 'gmbh', 'pa', 'assn', 'group', 'and', 'pc', 'hotel', 'pharmacy', 'assoc', 'hotels', 'plc', 'associates', 'inc', 'pllc', 'association', 'incorporated', 'restaurant', 'bank', 'international', 'sa', 'bv', 'intl', 'sales', 'co', 'limited', 'service', 'comp', 'llc', 'services', 'company', 'llp', 'store', 'corp', 'lp', 'svcs', 'corporation', 'travel', 'dmd', 'manufacturing', 'unlimited', 'enterprises', 'mfg']\n",
    "    \n",
    "    filtered_sentence = [w for w in tokenize if not w in stopwords] \n",
    "    tokens.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [''.join(i) for i in tokens]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the newly created keys to a new column 'newKeys'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2['newKeys'] = keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to kind of show up until now the work that has been done we can see below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = db2['CustomerName']\n",
    "postcode = db2['AddressLine4']         \n",
    "print('Debtor Name:', '\"', name[3], '\"')\n",
    "print('Debtor PostCode:', '\"', postcode[3], '\"')\n",
    "print('Debtor Name + PostCode:', '\"', name_pc2[3], '\"')\n",
    "print('Normalised Tokens:', '\"', tokens[3], '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will re run our code that does the left join on our two databases and see what difference it makes in terms of identifying the records in our first database to be anywhere else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_join = pd.merge(db1, db2, on='newKeys', how='left')\n",
    "\n",
    "inner_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can analyse there is an extra match since the three entities we have intentionally created to prove our concept is: \n",
    "- RuhiBegum&Co\n",
    "- Sohiel And Family - Lion\"s Den\n",
    "- BMW Group\n",
    "\n",
    "we have purposefully created a minor difference in the postcode for 'Sohiel And Family...'  and therefore we will explore this through fuzzy matching since it is not identical and has not been picked up.\n",
    "\n",
    "This was a first simple approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy Partial Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Fuzz packages that will allow us to do the fuzzy partial matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable to compare the two columns hierarchically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = pd.MultiIndex.from_product([db1['newKeys'], db2['newKeys']]).to_series()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function creates our comparisons of newKeys for four different fuzz methods: \n",
    "- fuzz.ratio\n",
    "- fuzz.partial_ratio\n",
    "- fuzz.token_sort_ratio\n",
    "- fuzz.token_set_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(tup):\n",
    "#return pd.Series([fuzz.ratio(*tup), fuzz.token_sort_ratio(*tup)], ['ratio', 'token'])\n",
    "    ratio = fuzz.ratio(*tup)\n",
    "    partial = fuzz.partial_ratio(*tup)\n",
    "    token = fuzz.token_sort_ratio(*tup)\n",
    "    sett = fuzz.token_set_ratio(*tup)\n",
    "    return pd.Series([ratio, partial, token, sett], ['ratio', 'partial_ratio', 'token_sort_ratio', 'token_set_ratio'])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the comparisons in a table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table_of_comparisons = compare.apply(metrics)\n",
    "table_of_comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not need to acknowledge every single comparison, we set a threshold such that if two keys have a score greater or equal to 80 then it is one (same) entity. \n",
    "Our results of this are shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_comparisons[(table_of_comparisons['ratio'] >= 80) & (table_of_comparisons['partial_ratio'] >= 80) & (table_of_comparisons['token_sort_ratio'] >= 80) & (table_of_comparisons['token_set_ratio'] >= 80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_partial(tup):\n",
    "    token = fuzz.token_sort_ratio(*tup)\n",
    "    return pd.Series([token], ['token_sort_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comp = compare.apply(fuzzy_partial)\n",
    "comp[(comp['token_sort_ratio'] >= 80)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test the effect of what we have so far against a third database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above, we write from the retrieved data of Mock3Customer into Mock3.dbo.test3 and read it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDB(database):\n",
    "    conn = connectToDB('DESKTOP-7CLEOQN\\SQLEXPRESS', database)\n",
    "    query =  \"SELECT * FROM dbo.test3\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply execute a print statement just to verify we have loaded in the table correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db3 = readDB('Mock3')\n",
    "print(db3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, Create a new column in the DataFrame which concatenates the CustomerName and the corresponding Postal Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db3['name_postcode'] = db3[['CustomerName', 'PostCode']].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will do a left join on our target database (Mock) against db3.\n",
    "At this point, we expect to see zero joins since no Customer names are identical. \n",
    "However, we know through intentionally producing errors the entities that are same in both Database1 and Database3 are: \n",
    "- RuhiBegum&Co \n",
    "- Sohiel And Family - Lion's Den\n",
    "- D.I. Cash And Carry \n",
    "- Bongos Dongos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_join = pd.merge(db1, db3, on='CustomerName', how='left')\n",
    "\n",
    "inner_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will carry out the preprocessing steps we have produced. \n",
    "The following code performs tokenisation, which has normalisation embedded on line 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_pc = db3['name_postcode']\n",
    "tokens = []\n",
    "\n",
    "for i in name_pc:\n",
    "    no_punc = strip_punctuation(i)\n",
    "    tokenize = word_tokenize(no_punc) #tokenization from the NLTK library\n",
    "    \n",
    "    #LIST of all company/customer name stopwords \n",
    "    stopwords = ['agency', 'gmbh', 'pa', 'assn', 'group', 'and', 'pc', 'hotel', 'pharmacy', 'assoc', 'hotels', 'plc', 'associates', 'inc', 'pllc', 'association', 'incorporated', 'restaurant', 'bank', 'international', 'sa', 'bv', 'intl', 'sales', 'co', 'limited', 'service', 'comp', 'llc', 'services', 'company', 'llp', 'store', 'corp', 'lp', 'svcs', 'corporation', 'ltd', 'travel', 'dmd', 'manufacturing', 'unlimited', 'enterprises', 'mfg']\n",
    "    \n",
    "    filtered_sentence = [w for w in tokenize if not w in stopwords] \n",
    "    tokens.append(filtered_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pre-processing steps above, we produce our new keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [''.join(i) for i in tokens]    \n",
    "db3['newKeys'] = keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now perform an inner join simply after having produced newKeys. There is no partial matching at this point yet.\n",
    "For this reason, we only expect to see \n",
    "- D.I. Cash And Carry\n",
    "\n",
    "join since the keys are now identical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_join = pd.merge(db1, db3, on='newKeys', how='left')\n",
    "\n",
    "inner_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use our methods created for Fuzzy Partial Matching. \n",
    "\n",
    "Remember we apply a threshold of >=80 since this accommodates greater spelling flaws etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = pd.MultiIndex.from_product([db1['newKeys'], db3['newKeys']]).to_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 100000\n",
    "table_of_comparisons = compare.apply(metrics)\n",
    "table_of_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comp = compare.apply(fuzzy_partial)\n",
    "comp[(comp['token_sort_ratio'] >= 80)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us validate this one more time by retrieving entities that exist in Database 2 and Database 3. \n",
    "I jave purposely created more entities that should be retrieved. \n",
    "We should retrieve:\n",
    "- RuhiBegum&Co \n",
    "- Sohiel And Family - Lion's Den\n",
    "- Barclays Bank - Birmingham \n",
    "- Rojers Savouries\n",
    "- Sunpride\n",
    "- Lidl Cash And Carry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = pd.MultiIndex.from_product([db2['newKeys'], db3['newKeys']]).to_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 100000\n",
    "table_of_comparisons = compare.apply(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = compare.apply(fuzzy_partial)\n",
    "comp[(comp['token_sort_ratio'] >= 80)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see again, fuzzy partial matching accomodates for spelling errors, were characters are physically missing etc. These are features Fuzz.token_sort_ratio deals with better than partial_ratio but further research into different methods is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future consideration the necessity of address false positives/false negatives remains essential.\n",
    "For example, we can see cashcarry make bulk of the sequence for the key 'lidlcashcarrysy128gj' therefore, another entry also retains this phrase and therefore identified as similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp[(comp['token_sort_ratio'] >= 50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You for listening"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
